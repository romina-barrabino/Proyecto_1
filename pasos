##Proyecto_parte_1
#Cree una API desde la pagina https://home.openweathermap.org/
#Instalo SQL Microsoft Server y creo una carpeta llamada weather_db
#Clono el repositorio github
#a)Creo una carpeta en la computadora llamada Proyecto_1_2025 y la abro en el visual studio
#b)Descargo git desde https://git-scm.com/downloads/win
#c)Verifico la version de git 
git --version
#d)Para clonar use https debido a un problema de configuracion con SSH
git clone https://github.com/franncardenas/weather_project.git
#e)Creo la carpeta en visual code llamada requirements.txt y le agrego las instalaciones requeridas

requests==2.32.3
pandas==2.2.3
sqlalchemy==2.0.36 #para hacer consultas
python-dotenv==1.0.1 #para crear env
google-cloud-bigquery==3.26.0
google-auth==2.35.0
pyarrow==18.0.0
pyodbc

#Instalo las librerias de requirements.txt
pip install -r requirements.txt
#Creo un archivo ".env" para guardar la API y le agrego el siguiente script
OPENWEATHER_API_KEY=19da0f4b67274245671f080a277d4972
#Instalo la libreria python-dotenv
pip install python-dotenv
#En la carpeta extract_transform_load.py escribir el script de extraccion y carga en la base de datos:

#Librerias
from dotenv import load_dotenv
import os
import requests
import pyodbc

load_dotenv()

#Parámetros
API_KEY = os.getenv('OPENWEATHER_API_KEY')
COUNTRY = 'Argentina'
SQL_SERVER_CONFIG = {
    'DRIVER': '{ODBC Driver 17 for SQL Server}',
    'SERVER': 'localhost',
    'DATABASE': 'weather_db',
    'UID': 'sa',
    'PWD': 'simpleplan1994'
}

#Extracción de datos
def extraer_datos(api_key, country):
    url = f'https://api.openweathermap.org/data/2.5/weather?q={country}&appid={api_key}&units=metric'
    response = requests.get(url)
    data = response.json()
    return data

#Transformación de datos
def transformar_datos(data):
    try:
        return {
            'country': data['sys']['country'],
            'temperature': data['main']['temp'],
            'humidity': data['main']['humidity']
        }
    except KeyError as e:
        raise Exception(f"Se produjo un error en: {e}")
    
#Carga a SQL Server
def cargar_datos(datos, config):
    conn_str = (
        f"DRIVER={config['DRIVER']};"
        f"SERVER={config['SERVER']};"
        f"DATABASE={config['DATABASE']};"
        f"UID={config['UID']};"
        f"PWD={config['PWD']};"
    )
    conn = pyodbc.connect(conn_str)
    cursor = conn.cursor()

    insert_query = """
        INSERT INTO WeatherData (country, temperature, humidity)
        VALUES (?, ?, ?)
    """
    cursor.execute(insert_query, (
        datos['country'],
        datos['temperature'],
        datos['humidity']
    ))

    conn.commit()
    cursor.close()
    conn.close()
    print(f" Datos del clima en {datos['country']} cargados correctamente.")

#Proceso ETL completo
def ejecutar_etl():
    datos_extraidos = extraer_datos(API_KEY,COUNTRY)
    datos_transformados = transformar_datos(datos_extraidos)
    cargar_datos(datos_transformados, SQL_SERVER_CONFIG)
    
#Verificacion:
#a)Ejecurtar una consulta en SQL

##Proyecto_parte_2
#b)Instale la biblioteca de Google BigQuery en su entorno virtual
#c)Crea un script Python para cargar datos desde SQLserver a BigQuery.
#d)Función para extraer los datos de SQL Server. mi. Función para cargar los datos en BigQuery.
#e)Proceso ETL en BigQuery. 
#f)Configura las credenciales para acceder a Google Cloud siguiendo esta guía .

Paso a paso para desarrollar el proyecto: Opcional: Automatización y Monitoreo Si deseas automatizar este proceso, podrías usar un cron job en Linux o el Programador de Tareas en Windows para ejecutar el script en intervalos regulares.
